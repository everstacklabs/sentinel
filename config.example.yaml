# Model Updater Configuration
# Copy to config.yaml and customize for your environment.

# Path to the model-catalog directory (relative or absolute)
catalog_path: "../model-catalog"

# Cache settings
cache_dir: "~/.cache/sentinel"
cache_ttl: "1h"

# Providers to sync (default: all registered)
providers:
  - openai

# Source types to use for discovery
sources:
  - api

# Dry run mode: show changes without writing
dry_run: false

# Disable caching
no_cache: false

# Risk mode: "strict" (default) or "relaxed"
# strict: enforces all risk gates
# relaxed: creates normal PRs even when risk thresholds are exceeded
risk_mode: "strict"

# Log level: debug, info, warn, error
log_level: "info"

# GitHub settings (for PR creation)
github:
  # token: set via GITHUB_TOKEN env var
  owner: "everstacklabs"
  repo: "everstack"
  base_branch: "main"

# OpenAI settings
openai:
  # api_key: set via OPENAI_API_KEY env var
  base_url: "https://api.openai.com/v1"

# Anthropic settings
anthropic:
  # api_key: set via ANTHROPIC_API_KEY env var
  base_url: "https://api.anthropic.com/v1"

# Google/Gemini settings
google:
  # api_key: set via GEMINI_API_KEY env var
  base_url: "https://generativelanguage.googleapis.com/v1beta"

# LLM-as-Judge settings
# Evaluates changesets before writing, catching wrong capabilities,
# suspicious pricing, or nonsensical limits. Non-fatal â€” if the LLM
# call fails, the pipeline logs a warning and continues.
#
# Env var overrides:
#   SENTINEL_JUDGE_ENABLED    (default: false)
#   SENTINEL_JUDGE_PROVIDER   (default: anthropic)
#   SENTINEL_JUDGE_MODEL      (default: claude-sonnet-4-20250514)
#   SENTINEL_JUDGE_ON_REJECT  (default: draft)
#   SENTINEL_JUDGE_MAX_TOKENS (default: 4096)
#   ANTHROPIC_API_KEY
#   SENTINEL_ANTHROPIC_BASE_URL
judge:
  enabled: false
  provider: "anthropic"              # "anthropic" or "openai"
  model: "claude-sonnet-4-20250514"
  on_reject: "draft"                 # "draft" = mark PR as draft | "exclude" = remove rejected models
  max_tokens: 4096
